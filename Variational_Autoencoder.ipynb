{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":737475,"sourceType":"datasetVersion","datasetId":379764}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport datetime\nimport pandas as pd\nimport random\n\nfrom matplotlib import pyplot as plt\n\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Conv2DTranspose, Reshape\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\n\n\nfrom tensorflow.keras import layers\nimport tensorflow as tf\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n\n\ndef plot_losses(history):\n    plt.rcParams['figure.figsize'] = [20, 5]\n    f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n\n    ax1.set_title('Losses')\n    ax1.set_xlabel('epoch')\n    ax1.legend(loc=\"upper right\")\n    ax1.grid()\n    ax1.plot(history['loss'], label='Training loss')\n    ax1.plot(history['val_loss'], label='Validation loss')\n    ax1.legend()\n\n    ax2.set_title('Accuracy')\n    ax2.set_xlabel('epoch')\n    ax2.legend(loc=\"upper right\")\n    ax2.grid()\n    ax2.plot(history['accuracy'], label='Training accuracy')\n    ax2.plot(history['val_accuracy'], label='Validation accuracy')\n    ax2.legend()\n\n    plt.show()\n\ndef plot_resultados(model, carpeta, height=64, width=64, n=4):\n    \"\"\"\n    Muestra comparaciones entre imágenes originales y reconstruidas por el autoencoder.\n    \"\"\"\n    # Seleccionar imágenes aleatorias\n    archivos = os.listdir(carpeta)\n    archivos_img = random.sample(archivos, n)\n\n    # Cargar y normalizar las imágenes\n    imgs_originales = []\n    for nombre in archivos_img:\n        img = load_img(os.path.join(carpeta, nombre), target_size=(height, width))\n        img_array = img_to_array(img) / 255.0  # normalizar a [0,1]\n        imgs_originales.append(img_array)\n\n    imgs_originales = np.array(imgs_originales)\n\n    # Reconstruir con el modelo\n    imgs_reconstruidas = model.predict(imgs_originales)\n\n    # Mostrar resultados\n    plt.figure(figsize=(12, 6))\n    for i in range(n):\n        # Imagen original\n        ax = plt.subplot(2, n, i + 1)\n        plt.imshow(imgs_originales[i])\n        ax.set_title(\"Original\")\n        ax.axis(\"off\")\n\n        # Imagen reconstruida\n        ax = plt.subplot(2, n, i + 1 + n)\n        plt.imshow(imgs_reconstruidas[i])\n        ax.set_title(\"Reconstruida\")\n        ax.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:14:54.116059Z","iopub.execute_input":"2025-10-06T14:14:54.116324Z","iopub.status.idle":"2025-10-06T14:14:54.126221Z","shell.execute_reply.started":"2025-10-06T14:14:54.116306Z","shell.execute_reply":"2025-10-06T14:14:54.125521Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"# VAE","metadata":{}},{"cell_type":"markdown","source":"## Defining Hiperparameters","metadata":{}},{"cell_type":"code","source":"import os\n\nos.listdir(\"/kaggle/input\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:14:54.127397Z","iopub.execute_input":"2025-10-06T14:14:54.128087Z","iopub.status.idle":"2025-10-06T14:14:54.147108Z","shell.execute_reply.started":"2025-10-06T14:14:54.128063Z","shell.execute_reply":"2025-10-06T14:14:54.146403Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"['images']"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"height, width = 64, 64 # Image size, to fit in the competition size\nimage_dir = '/kaggle/input'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:14:54.147868Z","iopub.execute_input":"2025-10-06T14:14:54.148098Z","iopub.status.idle":"2025-10-06T14:14:54.155730Z","shell.execute_reply.started":"2025-10-06T14:14:54.148079Z","shell.execute_reply":"2025-10-06T14:14:54.154962Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"class Sampling(layers.Layer):\n    def call(self, inputs):\n        mu, log_var = inputs\n        sigma = tf.exp(0.5 * log_var)\n        epsilon = tf.random.normal(shape=tf.shape(mu))\n        return mu + sigma * epsilon\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:14:54.157483Z","iopub.execute_input":"2025-10-06T14:14:54.158159Z","iopub.status.idle":"2025-10-06T14:14:54.168432Z","shell.execute_reply.started":"2025-10-06T14:14:54.158138Z","shell.execute_reply":"2025-10-06T14:14:54.167765Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"input_img = keras.Input(shape=(height, width, 3))\n\n# Encoder\nx_enc = Conv2D(16, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(input_img)\nx_enc = Conv2D(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_enc)\nx_enc = Conv2D(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_enc)\nx_enc = Conv2D(128, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_enc)\nx_enc = Conv2D(128, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_enc)\nx_enc = Conv2D(256, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_enc)\nshape_before_flattening = keras.backend.int_shape(x_enc)[1:]\nx_enc = Flatten()(x_enc)\nx_enc = Dense(128, activation=\"relu\")(x_enc)\nx_enc = Dense(75, activation=\"relu\")(x_enc)\n\n# VAE: media y log-varianza\nlatent_dim = 64\nmu = Dense(latent_dim, name=\"mu\")(x_enc)\nlog_var = Dense(latent_dim, name=\"log_var\")(x_enc)\n\n# Muestreo\nz = Sampling()([mu, log_var])\n\n\n# Decoder\nx_dec = Dense(np.prod(shape_before_flattening), activation=\"relu\")(z)\nx_dec = Reshape(shape_before_flattening)(x_dec)\nx_dec = Conv2DTranspose(256, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_dec)\nx_dec = Conv2DTranspose(128, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_dec)\nx_dec = Conv2DTranspose(128, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_dec)\nx_dec = Conv2DTranspose(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_dec)\nx_dec = Conv2DTranspose(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_dec)\nx_dec = Conv2DTranspose(16, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_dec)\ndecoded = Conv2D(3, (3, 3), activation=\"sigmoid\", padding=\"same\")(x_dec)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:14:54.169056Z","iopub.execute_input":"2025-10-06T14:14:54.169264Z","iopub.status.idle":"2025-10-06T14:14:54.306869Z","shell.execute_reply.started":"2025-10-06T14:14:54.169249Z","shell.execute_reply":"2025-10-06T14:14:54.306326Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"We define the autoencoder, encoder and decoder","metadata":{}},{"cell_type":"code","source":"vae = keras.Model(input_img, decoded)\nencoder = keras.Model(input_img, z)\ndecoder = keras.Model(z, decoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:14:54.307605Z","iopub.execute_input":"2025-10-06T14:14:54.307857Z","iopub.status.idle":"2025-10-06T14:14:54.317214Z","shell.execute_reply.started":"2025-10-06T14:14:54.307836Z","shell.execute_reply":"2025-10-06T14:14:54.316463Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"opt = keras.optimizers.Adam(learning_rate=1e-3)\nvae.compile(optimizer=opt, loss=\"mse\", metrics = ['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:14:54.317938Z","iopub.execute_input":"2025-10-06T14:14:54.318118Z","iopub.status.idle":"2025-10-06T14:14:54.335208Z","shell.execute_reply.started":"2025-10-06T14:14:54.318104Z","shell.execute_reply":"2025-10-06T14:14:54.334541Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"## ImageDataGenerator\n\nTo fit with the competition, we will reshape images to 64x64","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\n# Create training and validation generators using the same seed\ntrain_generator = datagen.flow_from_directory(\n    image_dir,\n    target_size=(height, width),  \n    batch_size=256,\n    class_mode='input',\n    subset='training',\n    shuffle=True,\n    seed=2004  \n)\n\nval_generator = datagen.flow_from_directory(\n    image_dir,\n    target_size=(height, width),\n    batch_size=256,\n    class_mode='input',\n    subset='validation',\n    shuffle=False,\n    seed=2004\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:14:54.373314Z","iopub.execute_input":"2025-10-06T14:14:54.373540Z","iopub.status.idle":"2025-10-06T14:15:28.833327Z","shell.execute_reply.started":"2025-10-06T14:14:54.373497Z","shell.execute_reply":"2025-10-06T14:15:28.832603Z"}},"outputs":[{"name":"stdout","text":"Found 50852 images belonging to 1 classes.\nFound 12713 images belonging to 1 classes.\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"# Fit the autoencoder\nh = vae.fit(\n    train_generator,\n    epochs=20,\n    validation_data=val_generator\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:15:28.834651Z","iopub.execute_input":"2025-10-06T14:15:28.834884Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 485ms/step - accuracy: 0.7017 - loss: 0.0743 - val_accuracy: 0.7424 - val_loss: 0.0501\nEpoch 2/20\n\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 457ms/step - accuracy: 0.7438 - loss: 0.0465 - val_accuracy: 0.7427 - val_loss: 0.0483\nEpoch 3/20\n\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 468ms/step - accuracy: 0.7442 - loss: 0.0432 - val_accuracy: 0.7375 - val_loss: 0.0438\nEpoch 4/20\n\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 458ms/step - accuracy: 0.7395 - loss: 0.0410 - val_accuracy: 0.7438 - val_loss: 0.0418\nEpoch 5/20\n\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 472ms/step - accuracy: 0.7779 - loss: 0.0385 - val_accuracy: 0.8260 - val_loss: 0.0387\nEpoch 6/20\n\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434ms/step - accuracy: 0.8313 - loss: 0.0356","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"plot_losses(h.history)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Current timestamp\ntimestamp = datetime.datetime.now().strftime(\"%m_%d_%H:%M\")\n## Make sure everything saves correctly\nos.makedirs(\"models\", exist_ok=True)\nmodel_path = f\"models/VAE_{timestamp}.keras\"\n\n# Save the model\nvae.save(model_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_resultados(vae, \"/kaggle/input/images\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}