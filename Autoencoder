{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":737475,"sourceType":"datasetVersion","datasetId":379764}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport datetime\n\n\nfrom matplotlib import pyplot as plt\n\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Conv2DTranspose, Reshape\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:00:14.427103Z","iopub.execute_input":"2025-10-06T14:00:14.427901Z","iopub.status.idle":"2025-10-06T14:00:14.433552Z","shell.execute_reply.started":"2025-10-06T14:00:14.427871Z","shell.execute_reply":"2025-10-06T14:00:14.432730Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def plot_losses(history):\n    plt.rcParams['figure.figsize'] = [20, 5]\n    f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n\n    ax1.set_title('Losses')\n    ax1.set_xlabel('epoch')\n    ax1.legend(loc=\"upper right\")\n    ax1.grid()\n    ax1.plot(history['loss'], label='Training loss')\n    ax1.plot(history['val_loss'], label='Validation loss')\n    ax1.legend()\n\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef plot_resultados(model, carpeta, height=64, width=64, n=4):\n    \"\"\"\n    Muestra comparaciones entre imágenes originales y reconstruidas por el autoencoder.\n    \"\"\"\n    # Seleccionar imágenes aleatorias\n    archivos = os.listdir(carpeta)\n    archivos_img = random.sample(archivos, n)\n\n    # Cargar y normalizar las imágenes\n    imgs_originales = []\n    for nombre in archivos_img:\n        img = load_img(os.path.join(carpeta, nombre), target_size=(height, width))\n        img_array = img_to_array(img) / 255.0  # normalizar a [0,1]\n        imgs_originales.append(img_array)\n\n    imgs_originales = np.array(imgs_originales)\n\n    # Reconstruir con el modelo\n    imgs_reconstruidas = model.predict(imgs_originales)\n\n    # Mostrar resultados\n    plt.figure(figsize=(12, 6))\n    for i in range(n):\n        # Imagen original\n        ax = plt.subplot(2, n, i + 1)\n        plt.imshow(imgs_originales[i])\n        ax.set_title(\"Original\")\n        ax.axis(\"off\")\n\n        # Imagen reconstruida\n        ax = plt.subplot(2, n, i + 1 + n)\n        plt.imshow(imgs_reconstruidas[i])\n        ax.set_title(\"Reconstruida\")\n        ax.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Image analysis\n\nWe plot some of the images","metadata":{}},{"cell_type":"code","source":"# image_dir = '/kaggle/input/animefacedataset/images'\n\n# # Load image file paths\n# image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir)]\n\n# # Read images and collect their shapes\n# images = []\n# shapes = []\n# for file in image_files:\n#     img = cv2.imread(file)\n#     if img is not None:\n#         images.append(img)\n#         shapes.append(img.shape)\n\n# # Check if all shapes are the same\n# all_same_shape = all(shape == shapes[0] for shape in shapes)\n# print(\"All images same shape:\", all_same_shape)\n# print(\"Image shape (height, width, channels):\", shapes[0])\n\n# # Plot the first 4 images\n# fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n# for i in range(min(4, len(images))):\n#     img_rgb = cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB)\n#     axs[i].imshow(img_rgb)\n#     axs[i].axis('off')\n#     axs[i].set_title(f'Image {i+1}')\n# plt.tight_layout()\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:00:14.449742Z","iopub.execute_input":"2025-10-06T14:00:14.449961Z","iopub.status.idle":"2025-10-06T14:00:59.289438Z","shell.execute_reply.started":"2025-10-06T14:00:14.449936Z","shell.execute_reply":"2025-10-06T14:00:59.287737Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1867757555.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"# pd.Series(shapes).value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:00:59.289999Z","iopub.status.idle":"2025-10-06T14:00:59.290273Z","shell.execute_reply.started":"2025-10-06T14:00:59.290114Z","shell.execute_reply":"2025-10-06T14:00:59.290124Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Shapes.value_counts()\n```\n(92, 92, 3)      1636\n(96, 96, 3)      1556\n(94, 94, 3)      1534\n(95, 95, 3)      1530\n(90, 90, 3)      1516\n                 ... \n(87, 88, 3)         1\n(220, 220, 3)       1\n(126, 127, 3)       1\n(101, 102, 3)       1\n(181, 181, 3)       1\nName: count, Length: 181, dtype: int64\n```","metadata":{}},{"cell_type":"markdown","source":"# Autoencoder","metadata":{}},{"cell_type":"markdown","source":"## Defining Hiperparameters","metadata":{}},{"cell_type":"code","source":"height, width = 64, 64 # Image size, to fit in the competition size\nimage_dir = '/kaggle/input/animefacedataset'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:01:20.775208Z","iopub.execute_input":"2025-10-06T14:01:20.775963Z","iopub.status.idle":"2025-10-06T14:01:20.779506Z","shell.execute_reply.started":"2025-10-06T14:01:20.775937Z","shell.execute_reply":"2025-10-06T14:01:20.778617Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"input_img = keras.Input(shape=(height, width, 3))\n\n# Encoder\nx_enc = Conv2D(16, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(input_img)\nx_enc = Conv2D(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_enc)\nx_enc = Conv2D(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_enc)\nx_enc = Conv2D(128, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_enc)\nx_enc = Conv2D(128, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_enc)\nx_enc = Conv2D(256, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_enc)\nshape_before_flattening = keras.backend.int_shape(x_enc)[1:]  # Save shape for reshaping later\nx_enc = Flatten()(x_enc)\nx_enc = Dense(128, activation=\"relu\")(x_enc)\nx_enc = Dense(64, activation=\"relu\")(x_enc)\nencoded = Dense(32, activation=\"relu\")(x_enc)\n\n# Decoder\nx_dec = Dense(64, activation=\"relu\")(encoded)\nx_dec = Dense(128, activation=\"relu\")(x_dec)\nx_dec = Dense(np.prod(shape_before_flattening), activation=\"relu\")(x_dec)\nx_dec = Reshape(shape_before_flattening)(x_dec)\nx_dec = Conv2DTranspose(256, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_dec)\nx_dec = Conv2DTranspose(128, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_dec)\nx_dec = Conv2DTranspose(128, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_dec)\nx_dec = Conv2DTranspose(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_dec)\nx_dec = Conv2DTranspose(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_dec)\nx_dec = Conv2DTranspose(16, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x_dec)\ndecoded = Conv2D(3, (3, 3), activation=\"sigmoid\", padding=\"same\")(x_dec)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:01:21.705103Z","iopub.execute_input":"2025-10-06T14:01:21.705428Z","iopub.status.idle":"2025-10-06T14:01:21.841127Z","shell.execute_reply.started":"2025-10-06T14:01:21.705398Z","shell.execute_reply":"2025-10-06T14:01:21.840336Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"We define the autoencoder, encoder and decoder","metadata":{}},{"cell_type":"code","source":"autoencoder = keras.Model(input_img, decoded)\nencoder = keras.Model(input_img, encoded)\ndecoder = keras.Model(encoded, decoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:01:25.303192Z","iopub.execute_input":"2025-10-06T14:01:25.303509Z","iopub.status.idle":"2025-10-06T14:01:25.313474Z","shell.execute_reply.started":"2025-10-06T14:01:25.303484Z","shell.execute_reply":"2025-10-06T14:01:25.312589Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"opt = keras.optimizers.Adam(learning_rate=1e-3)\nautoencoder.compile(optimizer=opt, loss=\"mse\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:02:54.792021Z","iopub.execute_input":"2025-10-06T14:02:54.792677Z","iopub.status.idle":"2025-10-06T14:02:54.801590Z","shell.execute_reply.started":"2025-10-06T14:02:54.792651Z","shell.execute_reply":"2025-10-06T14:02:54.800823Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## ImageDataGenerator\n\nTo fit with the competition, we will reshape images to 64x64","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\n# Create training and validation generators using the same seed\ntrain_generator = datagen.flow_from_directory(\n    image_dir,\n    target_size=(height, width),  \n    batch_size=256,\n    class_mode='input',\n    subset='training',\n    shuffle=True,\n    seed=2004  \n)\n\nval_generator = datagen.flow_from_directory(\n    image_dir,\n    target_size=(height, width),\n    batch_size=256,\n    class_mode='input',\n    subset='validation',\n    shuffle=False,\n    seed=2004\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:02:58.423825Z","iopub.execute_input":"2025-10-06T14:02:58.424072Z","iopub.status.idle":"2025-10-06T14:03:21.385103Z","shell.execute_reply.started":"2025-10-06T14:02:58.424055Z","shell.execute_reply":"2025-10-06T14:03:21.384558Z"}},"outputs":[{"name":"stdout","text":"Found 50852 images belonging to 1 classes.\nFound 12713 images belonging to 1 classes.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"# Fit the autoencoder\nh = autoencoder.fit(\n    train_generator,\n    epochs=6,\n    validation_data=val_generator\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:08:44.365198Z","iopub.execute_input":"2025-10-06T14:08:44.365941Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/6\n\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 310ms/step - loss: 0.0398 - val_loss: 0.0410\nEpoch 2/6\n\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - loss: 0.0371","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"plot_losses(h.history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:06:58.566708Z","iopub.status.idle":"2025-10-06T14:06:58.566925Z","shell.execute_reply.started":"2025-10-06T14:06:58.566823Z","shell.execute_reply":"2025-10-06T14:06:58.566832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_dir=\"/kaggle/input/animefacedataset/images\"\nplot_resultados(autoencoder,image_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Current timestamp\ntimestamp = datetime.datetime.now().strftime(\"%m_%d_%H:%M\")\n## Make sure everything saves correctly\nos.makedirs(\"models\", exist_ok=True)\n\nmodel_path = f\"models/autoencoder_{timestamp}.keras\"\n\n# Save the model\nmodel.save(model_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:00:59.301053Z","iopub.status.idle":"2025-10-06T14:00:59.301315Z","shell.execute_reply.started":"2025-10-06T14:00:59.301210Z","shell.execute_reply":"2025-10-06T14:00:59.301220Z"}},"outputs":[],"execution_count":null}]}